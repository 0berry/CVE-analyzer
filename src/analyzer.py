#!/usr/bin/env python
# coding: utf8

import argparse
import os
import sys
import json
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding


# new entity label
LABELS = [
    "FUNCTION",
    "VERSION",
    "SOURCECODE",
    "DRIVER",
    "STRUCT",
    "VULNERABILITY",
    "CAPABILITY"
]


# ------------------------ PERFORMANCES ------------------------

def compute_performance_exact_match(performaces, annotations, entities):
    predictions = [[ent.start_char, ent.end_char, ent.label_] for ent in entities]
    for entry in annotations + predictions:
        if entry in annotations and entry in predictions:
            performaces["tp"] += 1
        elif entry in annotations and entry not in predictions:
            performaces["fn"] += 1
        elif entry not in annotations and entry in predictions:
            performaces["fp"] += 1
        else:
            performaces['tn'] += 1


def compute_precision(performaces):
    return float(performaces["tp"]) / (performaces["tp"] + performaces["fp"])


def compute_recall(performaces):
    return float(performaces["tp"]) / (performaces["tp"] + performaces["fn"])


def compute_f_measure(precision, recall):
    return 2*precision*recall / (precision + recall)

# ------------------------ END PERFORMANCES ------------------------


def get_dataset(dataset_path):
    if not os.path.exists(dataset_path):
        print("Dataset file {} not found".format(dataset_path))
        sys.exit(-1)
    with open(dataset_path, 'r') as dataset_f:
        dataset = json.load(dataset_f)
    return dataset


def get_train_and_test_sets(dataset_file):
    dataset = get_dataset(dataset_file)
    random.shuffle(dataset)
    split = len(dataset)*2/3
    return dataset[:split], dataset[split:]


def save_model(output_dir, model_name, nlp):
    output_dir = Path(output_dir)
    if not output_dir.exists():
        output_dir.mkdir()
    nlp.meta['name'] = model_name  # rename model
    nlp.to_disk(output_dir)
    print("Saved model to", output_dir)


def get_model(model):
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print("Loaded model '%s'" % model)
    else:
        nlp = spacy.blank('en')  # create blank Language class
        print("Created blank 'en' model")
    return nlp


def get_ner_component(nlp):
    # Add entity recognizer to model if it's not in the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner)
    # otherwise, get it, so we can add labels to it
    else:
        ner = nlp.get_pipe('ner')
    return ner


def get_optimizer(model, nlp):
    if model is None:
        optimizer = nlp.begin_training()
    else:
        # Note that 'begin_training' initializes the models, so it'll zero out
        # existing entity types.
        optimizer = nlp.entity.create_optimizer()
    return optimizer


def train(nlp, optimizer, n_iter, train_data):
    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        for _ in range(n_iter):
            random.shuffle(train_data)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(train_data, size=compounding(4., 32., 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)
            print('Losses', losses)


def test(nlp, test_data):
    exact_performance = {
        "tp": 0,
        "fp": 0,
        "fn": 0,
        "tn": 0
    }
    for description, annotations in test_data:
        doc = nlp(description)
        compute_performance_exact_match(exact_performance, annotations['entities'], doc.ents)
    precision = compute_precision(exact_performance)
    recall = compute_recall(exact_performance)
    f_measure = compute_f_measure(precision, recall)
    print
    print "-------------------------------------------"
    print "PERFORMANCES:"
    print
    print "Precision: {}".format(precision)
    print "Recall: {}".format(recall)
    print "F-measure: {}".format(f_measure)
    print
    print "-------------------------------------------"


def main(dataset_file, model, new_model_name, output_dir, n_iter):
    train_data, test_data = get_train_and_test_sets(dataset_file)

    nlp = get_model(model)
    ner = get_ner_component(nlp)

    if model is None:
        for label in LABELS:
            ner.add_label(label)
        optimizer = get_optimizer(model, nlp)
        train(nlp, optimizer, n_iter, train_data)

    if output_dir is not None:
        save_model(output_dir, new_model_name, nlp)

    test(nlp, test_data)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Train and test the CVE analyzer')
    parser.add_argument('dataset_file', type=str, metavar="<dataset.json>",
                        help="Location of the test set expressed in json format")
    parser.add_argument('-m', '--model', type=str, default=None, help="Path to a model that was previously save")
    parser.add_argument('-o', '--outputdir', type=str, default=None,
                        help='If this option is set the model is gonna be saved in the specified directory')
    parser.add_argument('-f', '--filename', type=str, default='model',  help="Name to give to the newly saved model")
    parser.add_argument('-n', '--niter', type=int, default=100, help='Number of iteration for the trainig part')
    args = parser.parse_args()

    main(args.dataset_file, args.model, args.filename, args.outputdir, args.niter)
