#!/usr/bin/env python
# coding: utf8

import argparse
import os
import sys
import json
import random
from pathlib import Path
import spacy
from spacy.util import minibatch, compounding


# new entity label
LABELS = [
    "FUNCTION",
    "VERSION",
    "SOURCECODE",
    "DRIVER",
    "STRUCT",
    "VULNERABILITY",
    "CAPABILITY"
]


def get_dataset(dataset_path):
    if not os.path.exists(dataset_path):
        print("Dataset file {} not found".format(dataset_path))
        sys.exit(-1)
    with open(dataset_path, 'r') as dataset_f:
        dataset = json.load(dataset_f)
    return dataset


def save_model(output_dir, model_name, nlp):
    output_dir = Path(output_dir)
    if not output_dir.exists():
        output_dir.mkdir()
    nlp.meta['name'] = model_name  # rename model
    nlp.to_disk(output_dir)
    print("Saved model to", output_dir)


def get_model(model):
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print("Loaded model '%s'" % model)
    else:
        nlp = spacy.blank('en')  # create blank Language class
        print("Created blank 'en' model")
    return nlp


def get_ner_component(nlp):
    # Add entity recognizer to model if it's not in the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner)
    # otherwise, get it, so we can add labels to it
    else:
        ner = nlp.get_pipe('ner')
    return ner


def get_optimizer(model, nlp):
    if model is None:
        optimizer = nlp.begin_training()
    else:
        # Note that 'begin_training' initializes the models, so it'll zero out
        # existing entity types.
        optimizer = nlp.entity.create_optimizer()
    return optimizer


def train(nlp, optimizer, n_iter, train_data):
    # get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        for _ in range(n_iter):
            random.shuffle(train_data)
            losses = {}
            # batch up the examples using spaCy's minibatch
            batches = minibatch(train_data, size=compounding(4., 32., 1.001))
            for batch in batches:
                texts, annotations = zip(*batch)
                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,
                           losses=losses)
            print('Losses', losses)


def test(nlp, test_data):
    doc = nlp(test_data)
    print("Entities in '%s'" % test_data)
    for ent in doc.ents:
        print(ent.label_, ent.text)


def main(train_set_file, model, new_model_name, output_dir, n_iter):
    """Set up the pipeline and entity recognizer, and train the new entity."""
    train_data = get_dataset(train_set_file)

    nlp = get_model(model)
    ner = get_ner_component(nlp)

    if model is None:
        for label in LABELS:
            ner.add_label(label)   # add new entity label to entity recognizer
        optimizer = get_optimizer(model, nlp)
        train(nlp, optimizer, n_iter, train_data)

    if output_dir is not None:
        save_model(output_dir, new_model_name, nlp)

    # test the trained model
    test_data = "The dvb_frontend_free function in drivers/media/dvb-core/dvb_frontend.c in the Linux kernel through 4.13.11 allows local users to cause a denial of service (use-after-free and system crash) or possibly have unspecified other impact via a crafted USB device. NOTE: the function was later renamed __dvb_frontend_free"
    test(nlp, unicode(test_data))


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Train and test the CVE analyzer')
    parser.add_argument('train_set_file', type=str, metavar="<train_set.json>",
                        help="Location of the training set expressed in json format")
    parser.add_argument('-m', '--model', type=str, default=None, help="Path to a model that was previously save")
    parser.add_argument('-o', '--outputdir', type=str, default=None,
                        help='If this option is set the model is gonna be saved in the specified directory')
    parser.add_argument('-f', '--filename', type=str, default='model',  help="Name to give to the newly saved model")
    parser.add_argument('-n', '--niter', type=int, default=100, help='Number of iteration for the trainig part')
    args = parser.parse_args()

    main(args.train_set_file, args.model, args.filename, args.outputdir, args.niter)
